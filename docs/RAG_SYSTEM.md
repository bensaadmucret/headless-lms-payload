# ğŸ§  SystÃ¨me RAG (Retrieval-Augmented Generation)

## ğŸ“‹ Vue d'Ensemble

Le systÃ¨me RAG transforme vos documents en une base de connaissances interrogeable sÃ©mantiquement. Il permet de :
- ğŸ” Rechercher des informations pertinentes dans vos documents
- ğŸ¯ Obtenir des rÃ©ponses contextuelles prÃ©cises
- ğŸ“Š Analyser le contenu de maniÃ¨re intelligente

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. EXTRACTION (Existant)                               â”‚
â”‚     pdfProcessor.ts â†’ Texte brut                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. CHUNKING (Nouveau)                                  â”‚
â”‚     chunkingService.ts                                  â”‚
â”‚     â†’ DÃ©coupe le texte en morceaux intelligents         â”‚
â”‚     â†’ 1000 chars par chunk, 200 chars overlap           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. EMBEDDINGS (Nouveau)                                â”‚
â”‚     embeddingService.ts                                 â”‚
â”‚     â†’ Transforme chaque chunk en vecteur numÃ©rique      â”‚
â”‚     â†’ Support OpenAI / HuggingFace / Local              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. VECTOR STORE (Nouveau)                              â”‚
â”‚     vectorStoreService.ts                               â”‚
â”‚     â†’ Stocke les vecteurs dans ChromaDB                 â”‚
â”‚     â†’ Permet la recherche sÃ©mantique                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Composants CrÃ©Ã©s

### 1. **chunkingService.ts**
DÃ©coupe intelligente du texte en morceaux optimaux.

**StratÃ©gies disponibles :**
- `standard` : DÃ©coupage rÃ©cursif avec sÃ©parateurs intelligents (par dÃ©faut)
- `chapters` : DÃ©coupage par chapitres dÃ©tectÃ©s automatiquement
- `fixed` : DÃ©coupage Ã  taille fixe (pour tests)

**Options :**
```typescript
{
  chunkSize: 1000,        // Taille de chaque chunk
  chunkOverlap: 200,      // Chevauchement entre chunks
  separators: ['\n\n', '\n', '. ', ' ', '']
}
```

### 2. **embeddingService.ts**
GÃ©nÃ©ration d'embeddings (vecteurs numÃ©riques).

**Providers supportÃ©s :**

| Provider | ModÃ¨le par dÃ©faut | Dimensions | CoÃ»t | Vitesse |
|----------|-------------------|------------|------|---------|
| **OpenAI** | text-embedding-3-small | 1536 | ğŸ’° Payant | âš¡ Rapide |
| **HuggingFace** | all-MiniLM-L6-v2 | 384 | ğŸ’° Payant | âš¡ Rapide |
| **Local** | Xenova/all-MiniLM-L6-v2 | 384 | âœ… Gratuit | ğŸ¢ Lent |

**Configuration :**
```bash
# .env
OPENAI_API_KEY=sk-...           # Pour OpenAI
HUGGINGFACE_API_KEY=hf_...      # Pour HuggingFace
# Pas de clÃ© nÃ©cessaire pour Local
```

### 3. **vectorStoreService.ts**
Stockage et recherche vectorielle avec ChromaDB.

**FonctionnalitÃ©s :**
- âœ… Stockage de chunks avec mÃ©tadonnÃ©es
- âœ… Recherche sÃ©mantique (similaritÃ© cosinus)
- âœ… Gestion de collections par document
- âœ… Recherche globale multi-documents
- âœ… Statistiques et monitoring

### 4. **ragWorker.ts**
Worker Bull qui orchestre le pipeline RAG complet.

**Fonctions principales :**
- `processRAGJob()` : Pipeline complet (chunking â†’ embeddings â†’ storage)
- `searchInDocument()` : Recherche dans un document spÃ©cifique
- `deleteDocumentRAG()` : Suppression des donnÃ©es RAG
- `getDocumentRAGStats()` : Statistiques d'un document

## ğŸš€ Installation

### 1. Installer les DÃ©pendances

```bash
pnpm install
```

Les dÃ©pendances suivantes ont Ã©tÃ© ajoutÃ©es Ã  `package.json` :
- `chromadb` : Base de donnÃ©es vectorielle
- `langchain` : Framework RAG
- `@langchain/core` : Core LangChain
- `@langchain/community` : IntÃ©grations communautaires
- `@langchain/openai` : IntÃ©gration OpenAI
- `@xenova/transformers` : Embeddings locaux

### 2. DÃ©marrer ChromaDB

**Option A : Docker (RecommandÃ©)**
```bash
docker run -d -p 8000:8000 chromadb/chroma
```

**Option B : Installation locale**
```bash
pip install chromadb
chroma run --host 0.0.0.0 --port 8000
```

### 3. Configurer les Variables d'Environnement

```bash
# .env
CHROMA_URL=http://localhost:8000

# Choisir un provider d'embeddings (au moins un)
OPENAI_API_KEY=sk-...           # Option 1 : OpenAI (recommandÃ©)
HUGGINGFACE_API_KEY=hf_...      # Option 2 : HuggingFace
# Option 3 : Local (pas de clÃ© nÃ©cessaire, mais plus lent)
```

## ğŸ’» Utilisation

### 1. Traiter un Document avec RAG

```typescript
import { addRAGJob } from './jobs/queue'

// AprÃ¨s l'extraction d'un PDF
const ragJob = await addRAGJob({
  type: 'rag-processing',
  documentId: 'doc_123',
  extractedText: '...texte extrait...',
  priority: 'normal',
  userId: 'user_456',
  chunkingOptions: {
    chunkSize: 1000,
    chunkOverlap: 200,
    strategy: 'standard'
  },
  embeddingOptions: {
    provider: 'openai',  // ou 'huggingface' ou 'local'
    model: 'text-embedding-3-small'
  }
})

console.log(`Job RAG crÃ©Ã©: ${ragJob.id}`)
```

### 2. Rechercher dans un Document

```typescript
import { searchInDocument } from './jobs/workers/ragWorker'

const results = await searchInDocument(
  'doc_123',
  'Quels sont les symptÃ´mes de la maladie ?',
  {
    topK: 5,           // Nombre de rÃ©sultats
    minScore: 0.5,     // Score minimum de pertinence
    embeddingProvider: 'openai'
  }
)

console.log('RÃ©sultats trouvÃ©s:', results.results)
// [
//   {
//     content: "Les symptÃ´mes incluent...",
//     score: 0.92,
//     chunkIndex: 15
//   },
//   ...
// ]
```

### 3. IntÃ©gration dans le Pipeline Existant

```typescript
// Dans extractionWorker.ts
import { addRAGJob } from '../queue'

// AprÃ¨s l'extraction rÃ©ussie
if (result.success) {
  // Lancer le traitement RAG en parallÃ¨le
  await addRAGJob({
    type: 'rag-processing',
    documentId: job.data.documentId,
    extractedText: result.extractedText,
    priority: job.data.priority,
    userId: job.data.userId,
  })
}
```

### 4. Obtenir les Statistiques

```typescript
import { getDocumentRAGStats } from './jobs/workers/ragWorker'

const stats = await getDocumentRAGStats('doc_123')

console.log(stats)
// {
//   success: true,
//   stats: {
//     collectionName: 'doc_doc_123',
//     chunksCount: 45,
//     exists: true
//   }
// }
```

## ğŸ”§ Configuration AvancÃ©e

### StratÃ©gies de Chunking

**1. Standard (RecommandÃ©)**
```typescript
chunkingOptions: {
  strategy: 'standard',
  chunkSize: 1000,
  chunkOverlap: 200
}
```
âœ… DÃ©coupage intelligent avec sÃ©parateurs
âœ… PrÃ©serve le contexte entre chunks
âœ… AdaptÃ© Ã  tous types de documents

**2. Par Chapitres**
```typescript
chunkingOptions: {
  strategy: 'chapters'
}
```
âœ… DÃ©tecte automatiquement les chapitres
âœ… IdÃ©al pour livres et cours structurÃ©s
âš ï¸ NÃ©cessite des marqueurs de chapitres

**3. Taille Fixe**
```typescript
chunkingOptions: {
  strategy: 'fixed',
  chunkSize: 500,
  chunkOverlap: 0
}
```
âœ… Rapide et prÃ©visible
âš ï¸ Peut couper au milieu de phrases

### Choix du Provider d'Embeddings

**OpenAI (RecommandÃ© pour Production)**
```typescript
embeddingOptions: {
  provider: 'openai',
  model: 'text-embedding-3-small'  // ou 'text-embedding-3-large'
}
```
âœ… Meilleure qualitÃ©
âœ… Rapide
ğŸ’° ~0.02$ / 1M tokens

**HuggingFace (Alternative)**
```typescript
embeddingOptions: {
  provider: 'huggingface',
  model: 'sentence-transformers/all-MiniLM-L6-v2'
}
```
âœ… Bonne qualitÃ©
âœ… Moins cher qu'OpenAI
âš ï¸ NÃ©cessite API key

**Local (DÃ©veloppement)**
```typescript
embeddingOptions: {
  provider: 'local',
  model: 'Xenova/all-MiniLM-L6-v2'
}
```
âœ… Gratuit
âœ… Pas de clÃ© API
âš ï¸ Plus lent (CPU)
âš ï¸ TÃ©lÃ©charge le modÃ¨le (~90MB)

## ğŸ“Š Monitoring

### VÃ©rifier l'Ã‰tat de ChromaDB

```typescript
import { vectorStoreService } from './jobs/services/vectorStoreService'

const isHealthy = await vectorStoreService.healthCheck()
console.log('ChromaDB status:', isHealthy ? 'âœ… Healthy' : 'âŒ Down')
```

### Lister les Collections

```typescript
const collections = await vectorStoreService.listCollections()
console.log('Collections:', collections)
// ['doc_doc_123', 'doc_doc_456', ...]
```

### Statistiques d'une Collection

```typescript
const stats = await vectorStoreService.getCollectionStats('doc_123')
console.log(stats)
// {
//   name: 'doc_doc_123',
//   count: 45,
//   exists: true
// }
```

## ğŸ› DÃ©pannage

### ChromaDB ne dÃ©marre pas

```bash
# VÃ©rifier si le port 8000 est libre
lsof -i :8000

# RedÃ©marrer ChromaDB
docker restart <container_id>
```

### Erreur "Cannot find module '@langchain/openai'"

```bash
# RÃ©installer les dÃ©pendances
rm -rf node_modules
pnpm install
```

### Embeddings locaux trop lents

```typescript
// Utiliser OpenAI ou HuggingFace Ã  la place
embeddingOptions: {
  provider: 'openai'  // Plus rapide
}
```

### Erreur de mÃ©moire avec gros documents

```typescript
// RÃ©duire la taille des chunks
chunkingOptions: {
  chunkSize: 500,  // Au lieu de 1000
  chunkOverlap: 100
}
```

## ğŸ“ˆ Performance

### Benchmarks

| Document | Pages | Chunks | Embeddings | Temps Total |
|----------|-------|--------|------------|-------------|
| 10 pages | 10 | 15 | OpenAI | ~5s |
| 50 pages | 50 | 75 | OpenAI | ~15s |
| 100 pages | 100 | 150 | OpenAI | ~30s |
| 100 pages | 100 | 150 | Local | ~2min |

### Optimisations

**1. Utiliser OpenAI pour les embeddings**
```typescript
embeddingOptions: { provider: 'openai' }
```

**2. Traiter en parallÃ¨le**
```typescript
// Le worker RAG tourne en parallÃ¨le du NLP et AI
```

**3. Ajuster la taille des chunks**
```typescript
// Plus de petits chunks = plus rapide mais moins de contexte
chunkingOptions: { chunkSize: 500 }
```

## ğŸ” SÃ©curitÃ©

### ClÃ©s API

```bash
# Ne JAMAIS commiter les clÃ©s dans le code
# Utiliser .env
OPENAI_API_KEY=sk-...
HUGGINGFACE_API_KEY=hf_...
```

### Isolation des DonnÃ©es

Chaque document a sa propre collection :
```
doc_doc_123  â†’ Collection pour document 123
doc_doc_456  â†’ Collection pour document 456
```

### Suppression des DonnÃ©es

```typescript
import { deleteDocumentRAG } from './jobs/workers/ragWorker'

await deleteDocumentRAG('doc_123')
// Supprime toutes les donnÃ©es RAG du document
```

## ğŸš€ Prochaines Ã‰tapes

1. **Installer les dÃ©pendances** : `pnpm install`
2. **DÃ©marrer ChromaDB** : `docker run -d -p 8000:8000 chromadb/chroma`
3. **Configurer .env** : Ajouter au moins une clÃ© API
4. **Tester** : Traiter un document avec RAG
5. **IntÃ©grer** : Ajouter au pipeline d'extraction existant

## ğŸ“š Ressources

- [LangChain Documentation](https://js.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [HuggingFace Models](https://huggingface.co/models?pipeline_tag=sentence-similarity)

## ğŸ†˜ Support

En cas de problÃ¨me :
1. VÃ©rifier les logs : `npm run workers:logs`
2. VÃ©rifier ChromaDB : `curl http://localhost:8000/api/v1/heartbeat`
3. VÃ©rifier les clÃ©s API dans `.env`
