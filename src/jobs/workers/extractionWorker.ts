/**
 * Worker pour l'extraction de contenu des documents
 */

import type { Job } from 'bull'
import type { ExtractionJob, ExtractionResult, ProcessingLog } from '../types'
import { ExtractionError } from '../types'
import { extractionQueue } from '../queue'
import { pdfProcessor } from '../processors/pdfProcessor'
import { epubProcessor } from '../processors/epubProcessor'
import { docxProcessor } from '../processors/docxProcessor'
import { txtProcessor } from '../processors/txtProcessor'
import { getPayloadInstance } from '../initPayload'
import { textToLexical } from '../../utils/lexicalUtils'

interface DocumentWithLogs {
  processingLogs?: string
  [key: string]: unknown
}

interface ChapterWithPages {
  title: string
  content?: string
  pageNumbers?: number[]
}

/**
 * Configuration du worker d'extraction
 */
const WORKER_CONFIG = {
  concurrency: 3, // 3 documents en parall√®le max
  limiter: {
    max: 10, // 10 jobs par minute max
    duration: 60 * 1000,
  },
}

/**
 * Processer l'extraction d'un document
 */
export async function processExtractionJob(job: Job<ExtractionJob>): Promise<ExtractionResult> {
  const { documentId, fileType, sourceFileUrl, collectionType = 'knowledge-base' } = job.data
  
  console.log(`üîç [Extraction] Starting extraction for document ${documentId} (${fileType})`)
  
  try {
    // Mettre √† jour le status du document
    await updateDocumentStatus(documentId, collectionType, 'extracting', 10, 'D√©but de l\'extraction...')
    
    let result: ExtractionResult
    
    // Choisir le bon processor selon le type de fichier
    switch (fileType) {
      case 'pdf':
        result = await pdfProcessor.extract(sourceFileUrl)
        break
      case 'epub':
        result = await epubProcessor.extract(sourceFileUrl)
        break
      case 'docx':
        result = await docxProcessor.extract(sourceFileUrl)
        break
      case 'txt':
        result = await txtProcessor.extract(sourceFileUrl)
        break
      default:
        throw new ExtractionError(`Type de fichier non support√©: ${fileType}`, fileType)
    }
    
    if (!result.success) {
      throw new ExtractionError(result.error || '√âchec de l\'extraction', fileType, result as any)
    }
    
    // Validation du contenu extrait
    if (!result.extractedText || result.extractedText.trim().length === 0) {
      throw new ExtractionError('Aucun contenu textuel trouv√© dans le document', fileType)
    }
    
    // Mise √† jour avec le contenu extrait
    await updateDocumentWithExtraction(documentId, collectionType, result)
    await updateDocumentStatus(documentId, collectionType, 'extracting', 100, 'Extraction termin√©e avec succ√®s')
    
    // Lancer automatiquement le job NLP
    await scheduleNextJobs(job.data, result)
    
    console.log(`‚úÖ [Extraction] Document ${documentId} extracted successfully (${result.metadata.wordCount} mots)`)
    
    return result
    
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue'
    console.error(`‚ùå [Extraction] Document ${documentId} failed:`, errorMessage)
    
    await updateDocumentStatus(documentId, collectionType, 'failed', 0, `√âchec extraction: ${errorMessage}`)
    
    // Re-lancer l'erreur pour que Bull la capture
    throw error
  }
}

/**
 * Mettre √† jour le status du document dans la base
 */
async function updateDocumentStatus(
  documentId: string,
  collectionType: 'media' | 'knowledge-base',
  status: ProcessingLog['step'],
  progress: number,
  message: string
) {
  try {
    console.log(`üìù [Status] Document ${documentId}: ${status} (${progress}%) - ${message}`)

    const payload = await getPayloadInstance()

    // Pour Media, on ne fait que logger (pas de champs processingStatus)
    if (collectionType === 'media') {
      console.log(`üìù [Status] Media document ${documentId} - pas de mise √† jour de statut`)
      return
    }

    // Pour Knowledge-base, on garde la logique existante
    // R√©cup√©rer le document pour concat√©ner les logs
    let existingLogs = ''
    try {
      const current = await payload.findByID({ collection: 'knowledge-base', id: documentId, depth: 0 })
      existingLogs = (current as unknown as DocumentWithLogs)?.processingLogs || ''
    } catch {}

    const newLogs = `${existingLogs ? existingLogs + '\n' : ''}[${new Date().toISOString()}] ${status} ${progress}% - ${message}`.slice(0, 50000)

    await payload.update({
      collection: 'knowledge-base',
      id: documentId,
      data: {
        processingStatus: status,
        lastProcessed: new Date().toISOString(),
        processingLogs: newLogs,
      } as any,
      overrideAccess: true,
    })
  } catch (error) {
    console.error(`‚ùå [Status] Failed to update document ${documentId}:`, error)
  }
}

/**
 * Mettre √† jour le document avec les r√©sultats d'extraction
 */
async function updateDocumentWithExtraction(documentId: string, collectionType: 'media' | 'knowledge-base', result: ExtractionResult) {
  try {
    console.log(`üìù [Update] Document ${documentId} extraction completed:`)
    console.log(`   - Text length: ${result.extractedText.length} characters`)
    console.log(`   - Word count: ${result.metadata.wordCount}`)
    console.log(`   - Language: ${result.metadata.language}`)
    if (result.chapters) {
      console.log(`   - Chapters: ${result.chapters.length}`)
    }

    const payload = await getPayloadInstance()

    if (collectionType === 'media') {
      // Pour Media : mettre √† jour le champ extractedContent (format texte brut)
      console.log(`üìù [Update] Mise √† jour Media ${documentId} avec contenu extrait`)
      
      await payload.update({
        collection: 'media',
        id: documentId,
        data: {
          extractedContent: result.extractedText || '',
        },
        overrideAccess: true,
      })
      
    } else {
      // Pour Knowledge-base : utiliser texte brut comme Media (temporairement)
      console.log(`üìù [Update] Mise √† jour Knowledge-base ${documentId} avec texte brut`)
      
      // Utiliser le m√™me format que Media
      const extractedContent = result.extractedText || ''

      // Convertir les chapitres si pr√©sents
      const chapters = (result.chapters || []).map((ch, idx) => ({
        chapterTitle: ch.title,
        chapterNumber: idx + 1,
        content: textToLexical(ch.content || ''),
        pageNumbers: (ch as ChapterWithPages).pageNumbers,
      }))

      const searchableContent = (result.extractedText || '').slice(0, 50000)

      await payload.update({
        collection: 'knowledge-base',
        id: documentId,
        data: {
          extractedContent, // üéØ Texte brut au lieu de Lexical
          searchableContent,
          ...(chapters.length > 0 ? { chapters } : {}),
          lastProcessed: new Date().toISOString(),
          processingStatus: 'completed',
        } as any,
        overrideAccess: true,
      })
    }
  } catch (error) {
    console.error(`‚ùå [Update] Failed to update document ${documentId} extraction:`, error)
  }
}

/**
 * Planifier les jobs suivants (NLP, IA)
 */
async function scheduleNextJobs(extractionData: ExtractionJob, extractionResult: ExtractionResult) {
  try {
    const { addNLPJob, addAIJob } = await import('../queue')
    
    // Job NLP
    const nlpJob = await addNLPJob({
      type: 'nlp-processing',
      documentId: extractionData.documentId,
      extractedText: extractionResult.extractedText,
      language: extractionResult.metadata.language === 'en' ? 'en' : 'fr',
      features: ['keywords', 'summary', 'entities'],
      priority: extractionData.priority,
      userId: extractionData.userId,
    })
    
    console.log(`üîó [Chain] Scheduled NLP job ${nlpJob.id} for document ${extractionData.documentId}`)
    
    // Job IA (enrichissement)
    const aiJob = await addAIJob({
      type: 'ai-enrichment',
      documentId: extractionData.documentId,
      contentType: 'medical',
      tasks: ['summary', 'concept-extraction', 'difficulty-assessment'],
      context: {
        medicalDomain: 'general',
        targetAudience: 'medical_students'
      },
      priority: extractionData.priority,
      userId: extractionData.userId,
    })
    
    console.log(`üîó [Chain] Scheduled AI job ${aiJob.id} for document ${extractionData.documentId}`)
    
  } catch (error) {
    console.error(`‚ö†Ô∏è [Chain] Failed to schedule next jobs for ${extractionData.documentId}:`, error)
    // Ne pas faire √©chouer le job principal pour √ßa
  }
}

// ===== D√âMARRAGE DU WORKER =====

/**
 * D√©marrer le worker d'extraction
 */
export function startExtractionWorker() {
  console.log('üöÄ [Worker] Starting extraction worker...')
  
  extractionQueue.process('extract-document', WORKER_CONFIG.concurrency, processExtractionJob)
  
  // Event handlers sp√©cifiques au worker
  extractionQueue.on('completed', (job) => {
    const duration = job.processedOn ? Date.now() - job.processedOn : 'unknown'
    console.log(`‚úÖ [ExtractionWorker] Job ${job.id} completed in ${duration}ms`)
  })
  
  extractionQueue.on('failed', (job, err) => {
    console.error(`‚ùå [ExtractionWorker] Job ${job?.id} failed after ${job?.attemptsMade} attempts:`, err.message)
  })
  
  extractionQueue.on('stalled', (job) => {
    console.warn(`‚ö†Ô∏è [ExtractionWorker] Job ${job.id} stalled, will be retried`)
  })
  
  console.log(`‚úÖ [Worker] Extraction worker started with concurrency ${WORKER_CONFIG.concurrency}`)
}

// Auto-start si ce fichier est ex√©cut√© directement
if (import.meta.url === `file://${process.argv[1]}`) {
  startExtractionWorker()
}